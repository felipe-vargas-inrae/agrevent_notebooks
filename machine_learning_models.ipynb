{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-driven approach to predict Biomass\n",
    "\n",
    "For understanding this notebook is neccesary to check the explore_imagen_analysis notebook. Here, machine learning models implemented in Pyspark are used to predict the final biomass.\n",
    "\n",
    "## Loading PySpark session\n",
    "\n",
    "Next lines represent the code to start spark session into the machine. Additionally, some functions are implemented to extract information from MongoDB and create PySpark Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "#%matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## init the spark session and impose the MongoDB connector\n",
    "MONGO_URI=\"mongodb://localhost:27017/iot_db\" \n",
    "my_spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .master('local[*]')\\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.3.2\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", MONGO_URI+\".phis_experiments\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def get_collection_mongodb(collection, pipeline=None) :\n",
    "    '''Get one collection from MongoDB database, the pipeline parameter is optional'''\n",
    "    options = my_spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "                                            .option(\"database\",\"iot_db\")\\\n",
    "                                            .option(\"collection\", collection)                                           \n",
    "    if pipeline is not None: \n",
    "        options.option(\"pipeline\", pipeline)\n",
    "    return options.load()\n",
    "\n",
    "def pipeline_angle(angle):\n",
    "    '''Pipeline send to MongoDB to extract the angle information'''\n",
    "    return  \"{'$match': {'angle':'%s' }}\"%(angle)\n",
    "\n",
    "   \n",
    "#stats functions\n",
    "def cal_correlation(df):\n",
    "    '''stats function to calculate the correlation between the columns, asumming that all are numeric'''\n",
    "    col_names = df.columns\n",
    "    features = df.rdd.map(lambda row: row[0:])\n",
    "\n",
    "    corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "    corr_df = pd.DataFrame(corr_mat)\n",
    "    corr_df.index, corr_df.columns = col_names, col_names\n",
    "    return corr_df\n",
    "\n",
    "def join_dataframes(df1,df2, f1_column, f2_column): \n",
    "    '''Apply join function to two spark dataframes'''\n",
    "    ta = df1.alias('ta')\n",
    "    tb = df2.alias('tb')\n",
    "    \n",
    "    if f1_column == f2_column: ## avoid repeat column in join result\n",
    "        df_join = ta.join(tb,[f1_column])\n",
    "    else:   \n",
    "        df_join = ta.join(tb, ta[f1_column] == tb[f2_column])\n",
    "    return df_join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring a scenery to run this notebook\n",
    "\n",
    "The scenery is a set of configurations aim to define how the notebook will predict the biomass. For example, the angle to explore, the feature extraction technique to use, the slots number\n",
    "\n",
    "Line by line will be explain this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True results/angle_AVG/slots_4/all\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# After running the data exploration notebook some plants contain many null rows > 23, in this list are all these plants. \n",
    "# potenicial plants that has problems during the growth season, it might be because camera errors, algortihms erros, human erros\n",
    "potencial_nulls= [\n",
    "    'http://www.phenome-fppn.fr/m3p/arch/2017/c17000795'\n",
    ",'http://www.phenome-fppn.fr/m3p/arch/2017/c17000469'\n",
    ",'http://www.phenome-fppn.fr/m3p/arch/2017/c17001536']\n",
    "\n",
    "\n",
    "## imagen analysis can be disaggregated by the camera angle \n",
    "# all the angles takes in phenoarch\n",
    "# 0 is removed because doesn't have heigth over pot \n",
    "ANGLES= [\"30\",\"60\",\"90\",\"120\",\"150\",\"180\",\"210\",\"240\",\"270\",\"300\",\"330\",\"AVG\"] \n",
    "\n",
    "# ANGLE is the constant that is used in the next lines to indentify the camera ANGLE, some element of ANGLES array\n",
    "ANGLE =  ANGLES[11] #240      \n",
    "\n",
    "LABEL_ML= \"label\" # column from dataset that indicates the output variable to predict, label=biomass\n",
    "METRIC_PERFORMANCE_ML=\"rmse\" # the metrict to select models into the crossvalition iterations\n",
    "ENTITY_URI_COL= \"plantURI\"# the entity that joins biomass Dataframe with imagenAnalysis Dataframe \n",
    "DATE_COL = \"dayOfYear\" # the column that indentifies the date value\n",
    "\n",
    "TIME_SERIES_COLLECTION= \"phis_imagen_analysis_explicit_angle\" # timeseries imagen analysis collection name\n",
    "SUMMARY_DATA = \"phis_biomass\" # biomass collection name\n",
    "\n",
    "# some features selection options, all is to use all the features, pca is to use pca technique, \n",
    "# naive is a two steps implementation, (1) get the most relevant features for biomass, (2) filter the correlated with the features selected\n",
    "FEATURE_SELECTION_ALL = \"all\" \n",
    "FEATURE_SELECTION_PCA = \"pca\"\n",
    "FEATURE_SELECTION_NAIVE = \"naive\"\n",
    "FEATURE_SELECTION_METHOD= FEATURE_SELECTION_ALL # configure the feature option in the notebook\n",
    "PCA_OPTION =\"4\" #[\"2\",\"3\",\"4\"]  the pca number of components\n",
    "\n",
    "if FEATURE_SELECTION_METHOD==FEATURE_SELECTION_PCA: # when PCA is used as method some columns names change in the pipeline\n",
    "    FEATURE_ML = 'features_pca'\n",
    "    LAST_FOLDER = FEATURE_SELECTION_PCA+\"_\"+PCA_OPTION  \n",
    "else :\n",
    "    FEATURE_ML = 'features_scaled'\n",
    "    LAST_FOLDER = FEATURE_SELECTION_METHOD # False when there is not used PCA\n",
    "\n",
    "## how many slots to use  \n",
    "SLOTS= \"4\"\n",
    "until_ndays=30 # get first n days from time series data\n",
    "split_ndays=8 # groups size of ndays days \n",
    "groups= [1,2,3,4] # groups label, suitable for performance improvenment \n",
    "\n",
    "ACTIVATE_SAMPLING = True # this line is to select 10% of dataset when the notebook is tested\n",
    "\n",
    "MODEL_TYPE = \"lr\" # first  run linear regression models to avoid kernel errors then run rf models\n",
    "\n",
    "\n",
    "FOLDER = \"results/angle_%s/slots_%s/%s\"%(ANGLE,SLOTS,LAST_FOLDER) # a folder to store the scenery \n",
    "isDirectory = os.path.isdir(FOLDER)\n",
    "print(isDirectory,FOLDER) ## validate that the folder exists\n",
    "\n",
    "# result_notebook is a dictionary to save the final result in a File\n",
    "result_notebook= {'angle':ANGLE}\n",
    "result_notebook['folder']= FOLDER\n",
    "result_notebook['start']=str(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining time-series data from MongoDB\n",
    "<i>Loadding collection imagenanalysesangle_aggregated_by_day</i>\n",
    "\n",
    "in this poin is used the function ``get_collection_mongodb``, which allows to get a MongoDB collection, the loaded collection is mapped to a Dataframe. Following, the relevant features are projected. Finally variablecode values are reshape to columns by invoking the ``pivot`` function. \n",
    "\n",
    "Since variable code is an arbitrary list of variable names, this pivot process gives flexibility to the pre-processing and allows to add other predictors, for example, a new information about the plant. The schema is thought to facilitate this called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plantURI</th>\n",
       "      <th>dayOfYear</th>\n",
       "      <th>Silk_area</th>\n",
       "      <th>convex_hull_area</th>\n",
       "      <th>convex_hull_perimeter</th>\n",
       "      <th>height</th>\n",
       "      <th>height_over_pot</th>\n",
       "      <th>height_under_pot</th>\n",
       "      <th>number_of_objects</th>\n",
       "      <th>object_sum_area</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.697875e+03</td>\n",
       "      <td>332.055457</td>\n",
       "      <td>127.875000</td>\n",
       "      <td>129.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.208333</td>\n",
       "      <td>1361.875000</td>\n",
       "      <td>83.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.151560e+04</td>\n",
       "      <td>628.237317</td>\n",
       "      <td>230.208333</td>\n",
       "      <td>231.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>3937.791667</td>\n",
       "      <td>154.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.449629e+04</td>\n",
       "      <td>1001.990808</td>\n",
       "      <td>371.333333</td>\n",
       "      <td>309.545455</td>\n",
       "      <td>68.363636</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>6594.958333</td>\n",
       "      <td>257.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.322690e+05</td>\n",
       "      <td>2676.527085</td>\n",
       "      <td>844.375000</td>\n",
       "      <td>809.545455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>58799.333333</td>\n",
       "      <td>863.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.825041e+05</td>\n",
       "      <td>2403.782312</td>\n",
       "      <td>798.291667</td>\n",
       "      <td>786.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>64758.083333</td>\n",
       "      <td>726.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.235576e+05</td>\n",
       "      <td>3292.860667</td>\n",
       "      <td>1133.750000</td>\n",
       "      <td>1087.818182</td>\n",
       "      <td>28.727273</td>\n",
       "      <td>6.875000</td>\n",
       "      <td>121217.708333</td>\n",
       "      <td>905.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.207864e+05</td>\n",
       "      <td>3546.773774</td>\n",
       "      <td>1356.020833</td>\n",
       "      <td>1264.565217</td>\n",
       "      <td>85.608696</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>181940.083333</td>\n",
       "      <td>903.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.273222e+06</td>\n",
       "      <td>4590.556989</td>\n",
       "      <td>1784.708333</td>\n",
       "      <td>1733.818182</td>\n",
       "      <td>59.636364</td>\n",
       "      <td>9.375000</td>\n",
       "      <td>239447.583333</td>\n",
       "      <td>1083.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.565414e+05</td>\n",
       "      <td>3310.599642</td>\n",
       "      <td>1196.833333</td>\n",
       "      <td>1201.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.833333</td>\n",
       "      <td>188696.875000</td>\n",
       "      <td>932.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.674061e+05</td>\n",
       "      <td>3564.382594</td>\n",
       "      <td>1271.833333</td>\n",
       "      <td>1250.636364</td>\n",
       "      <td>1.909091</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>217354.083333</td>\n",
       "      <td>1018.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.862826e+05</td>\n",
       "      <td>3710.928762</td>\n",
       "      <td>1346.979167</td>\n",
       "      <td>1187.772727</td>\n",
       "      <td>126.272727</td>\n",
       "      <td>5.604167</td>\n",
       "      <td>129763.229167</td>\n",
       "      <td>1064.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>160</td>\n",
       "      <td>81464.0</td>\n",
       "      <td>1.265385e+06</td>\n",
       "      <td>4952.160979</td>\n",
       "      <td>2172.187500</td>\n",
       "      <td>1933.818182</td>\n",
       "      <td>248.500000</td>\n",
       "      <td>10.291667</td>\n",
       "      <td>174676.562500</td>\n",
       "      <td>968.104167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.617521e+03</td>\n",
       "      <td>300.542132</td>\n",
       "      <td>127.291667</td>\n",
       "      <td>131.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>1524.791667</td>\n",
       "      <td>62.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.815990e+05</td>\n",
       "      <td>1731.332179</td>\n",
       "      <td>634.937500</td>\n",
       "      <td>634.772727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>28985.354167</td>\n",
       "      <td>481.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.696247e+05</td>\n",
       "      <td>2363.182142</td>\n",
       "      <td>745.416667</td>\n",
       "      <td>717.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>55896.208333</td>\n",
       "      <td>724.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.740864e+05</td>\n",
       "      <td>2354.375097</td>\n",
       "      <td>708.750000</td>\n",
       "      <td>693.590909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>64309.708333</td>\n",
       "      <td>791.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.511607e+05</td>\n",
       "      <td>2671.690839</td>\n",
       "      <td>851.875000</td>\n",
       "      <td>820.909091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.541667</td>\n",
       "      <td>65952.208333</td>\n",
       "      <td>820.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.635971e+05</td>\n",
       "      <td>2688.427290</td>\n",
       "      <td>930.458333</td>\n",
       "      <td>925.818182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.458333</td>\n",
       "      <td>71364.375000</td>\n",
       "      <td>796.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.113399e+05</td>\n",
       "      <td>3103.280718</td>\n",
       "      <td>1020.291667</td>\n",
       "      <td>987.863636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.125000</td>\n",
       "      <td>99497.937500</td>\n",
       "      <td>995.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>http://www.phenome-fppn.fr/m3p/arch/2017/c1700...</td>\n",
       "      <td>142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.339439e+06</td>\n",
       "      <td>4554.102540</td>\n",
       "      <td>1727.208333</td>\n",
       "      <td>1709.363636</td>\n",
       "      <td>6.636364</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>222917.083333</td>\n",
       "      <td>1130.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             plantURI  dayOfYear  Silk_area  \\\n",
       "0   http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        103        NaN   \n",
       "1   http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        107        NaN   \n",
       "2   http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        110        NaN   \n",
       "3   http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        122        NaN   \n",
       "4   http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        124        NaN   \n",
       "5   http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        130        NaN   \n",
       "6   http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        136        NaN   \n",
       "7   http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        138        NaN   \n",
       "8   http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        139        NaN   \n",
       "9   http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        141        NaN   \n",
       "10  http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        144        NaN   \n",
       "11  http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        160    81464.0   \n",
       "12  http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        104        NaN   \n",
       "13  http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        117        NaN   \n",
       "14  http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        122        NaN   \n",
       "15  http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        123        NaN   \n",
       "16  http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        123        NaN   \n",
       "17  http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        124        NaN   \n",
       "18  http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        128        NaN   \n",
       "19  http://www.phenome-fppn.fr/m3p/arch/2017/c1700...        142        NaN   \n",
       "\n",
       "    convex_hull_area  convex_hull_perimeter       height  height_over_pot  \\\n",
       "0       4.697875e+03             332.055457   127.875000       129.636364   \n",
       "1       2.151560e+04             628.237317   230.208333       231.090909   \n",
       "2       5.449629e+04            1001.990808   371.333333       309.545455   \n",
       "3       4.322690e+05            2676.527085   844.375000       809.545455   \n",
       "4       3.825041e+05            2403.782312   798.291667       786.000000   \n",
       "5       7.235576e+05            3292.860667  1133.750000      1087.818182   \n",
       "6       8.207864e+05            3546.773774  1356.020833      1264.565217   \n",
       "7       1.273222e+06            4590.556989  1784.708333      1733.818182   \n",
       "8       7.565414e+05            3310.599642  1196.833333      1201.363636   \n",
       "9       8.674061e+05            3564.382594  1271.833333      1250.636364   \n",
       "10      8.862826e+05            3710.928762  1346.979167      1187.772727   \n",
       "11      1.265385e+06            4952.160979  2172.187500      1933.818182   \n",
       "12      3.617521e+03             300.542132   127.291667       131.181818   \n",
       "13      1.815990e+05            1731.332179   634.937500       634.772727   \n",
       "14      3.696247e+05            2363.182142   745.416667       717.363636   \n",
       "15      3.740864e+05            2354.375097   708.750000       693.590909   \n",
       "16      4.511607e+05            2671.690839   851.875000       820.909091   \n",
       "17      4.635971e+05            2688.427290   930.458333       925.818182   \n",
       "18      6.113399e+05            3103.280718  1020.291667       987.863636   \n",
       "19      1.339439e+06            4554.102540  1727.208333      1709.363636   \n",
       "\n",
       "    height_under_pot  number_of_objects  object_sum_area        width  \n",
       "0           0.000000           2.208333      1361.875000    83.875000  \n",
       "1           0.000000           1.375000      3937.791667   154.791667  \n",
       "2          68.363636           3.666667      6594.958333   257.583333  \n",
       "3           0.000000           3.416667     58799.333333   863.916667  \n",
       "4           0.000000           5.583333     64758.083333   726.875000  \n",
       "5          28.727273           6.875000    121217.708333   905.625000  \n",
       "6          85.608696           5.000000    181940.083333   903.312500  \n",
       "7          59.636364           9.375000    239447.583333  1083.791667  \n",
       "8           0.000000           7.833333    188696.875000   932.083333  \n",
       "9           1.909091           5.833333    217354.083333  1018.250000  \n",
       "10        126.272727           5.604167    129763.229167  1064.291667  \n",
       "11        248.500000          10.291667    174676.562500   968.104167  \n",
       "12          0.000000           1.083333      1524.791667    62.833333  \n",
       "13          0.000000           4.500000     28985.354167   481.375000  \n",
       "14          0.000000           4.625000     55896.208333   724.916667  \n",
       "15          0.000000           5.500000     64309.708333   791.708333  \n",
       "16          0.000000           5.541667     65952.208333   820.375000  \n",
       "17          0.000000           4.458333     71364.375000   796.750000  \n",
       "18          0.000000           6.125000     99497.937500   995.458333  \n",
       "19          6.636364           8.166667    222917.083333  1130.250000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from hour to day transformacion\n",
    "def get_by_angle(angle):\n",
    "    \n",
    "    if ANGLE ==\"AVG\":\n",
    "        pipeline = None\n",
    "    else :\n",
    "        pipeline = pipeline_angle(angle)\n",
    "    \n",
    "    df_images_anaysis = get_collection_mongodb( TIME_SERIES_COLLECTION ,pipeline)\n",
    "    \n",
    "    \n",
    "    df_images_anaysis = df_images_anaysis.filter(~df_images_anaysis[ENTITY_URI_COL].isin(potencial_nulls))\n",
    "    df_images_anaysis = df_images_anaysis.select(ENTITY_URI_COL, DATE_COL, \"variableCode\", \"value\")\n",
    "    df_images_anaysis = df_images_anaysis.groupBy(ENTITY_URI_COL, DATE_COL).pivot(\"variableCode\").mean(\"value\")\n",
    "    #df_images_anaysis = df_images_anaysis.orderBy(ENTITY_URI_COL,DATE_COL)\n",
    "    \n",
    "    result_notebook['columns_images']=df_images_anaysis.columns\n",
    "    \n",
    "    #display(df_images_anaysis.limit(20).toPandas())\n",
    "    #df_images_anaysis.show(200)\n",
    "    return df_images_anaysis\n",
    "    \n",
    "df_select_angle=get_by_angle(ANGLE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing time-series data \n",
    "\n",
    "<i>Apply pyspark transformations/actions to aggregate time-series data</i>\n",
    "\n",
    "Create a window lead by plantURI and dayOfYear fields, this window represents a plant growth season,\n",
    "the row_number is for numbering  the days inside the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import *\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "# porcion del dataset donde se realiza la secuencia, si hay dos ventanas se repetira la posicion 1 para cada ventana\n",
    "windowSpec = Window.partitionBy(ENTITY_URI_COL).orderBy(DATE_COL)\n",
    "# definción de transformación \n",
    "df_grp_img_analysis_row_number= df_select_angle.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "\n",
    "df_grp_img_analysis_row_number_biomass=df_grp_img_analysis_row_number.select(ENTITY_URI_COL,'row_number',\n",
    "                                              'convex_hull_area','height_over_pot','width','object_sum_area')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the number of slots and filter time-series data\n",
    "\n",
    "In this process, having the number of slots, a new column is defined to identify the group for each row,\n",
    "this column will be used for grouping. Second line is about to filter the time-series data, using\n",
    "the first ``until_days``. Finally, using the ```pivot``` function time-series data is aggregated, transforming\n",
    "one variable into n variables equivalent to the number of slots. The result is a dataframe with the same granularity of  summary data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_notebook['slots']={'until':until_ndays,'split':split_ndays}\n",
    "df_grp_img_analysis_row_number_biomass= df_grp_img_analysis_row_number_biomass\\\n",
    "                                                                    .withColumn(\"split_ndays_group\",\\\n",
    "                                                                      F.ceil(F.col(\"row_number\") /split_ndays))\n",
    "\n",
    "## filter to get first days \n",
    "df_grp_img_analysis_row_number_biomass_first_ndays=df_grp_img_analysis_row_number_biomass[F.col(\"row_number\")<=until_ndays]\n",
    "\n",
    "# applied pivot exclusively to certainty columns\n",
    "METHOD_AGG= 'avg'\n",
    "exprsPivoted = {\"convex_hull_area\":METHOD_AGG,'height_over_pot':METHOD_AGG,'width':METHOD_AGG ,\"object_sum_area\":METHOD_AGG}\n",
    "\n",
    "df_grp_img_analysis_row_number_biomass_split= df_grp_img_analysis_row_number_biomass_first_ndays\\\n",
    "                            .groupBy('plantURI').pivot('split_ndays_group',groups).agg(exprsPivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining biomass and time-series imagen analysis data\n",
    "\n",
    "In this process is created the biomass Dataframe, again invoking the ``get_collection_mongodb``function, some features are projected. In this way, a dataset is created by joining biomass Dataframe with time-series imagen analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_biomasa= get_collection_mongodb(SUMMARY_DATA)\n",
    "excel_biomasa= excel_biomasa.select('plantURI','Treatment','Biomass(gramos_pesofresco)')\n",
    "df_dataset = join_dataframes(df_grp_img_analysis_row_number_biomass_split,excel_biomasa,ENTITY_URI_COL,ENTITY_URI_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the dataset\n",
    "\n",
    "First will be deleted all the records where biomass field is null, the biomass field is casting to double, in order to manipulate as a number, which is required in regression problems. Finally, the flag ``ACTIVATE_SAMPLING`` is checked to know if the dataset must be reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows with no biomass value\n",
    "df_dataset = df_dataset.na.drop(subset=['Biomass(gramos_pesofresco)'])\n",
    "\n",
    "df_dataset=df_dataset.withColumn(\"biomass\", df_dataset[\"Biomass(gramos_pesofresco)\"].cast(\"double\"))\n",
    "\n",
    "df_dataset= df_dataset.drop('Biomass(gramos_pesofresco)')\n",
    "\n",
    "if ACTIVATE_SAMPLING:\n",
    "    df_dataset= df_dataset.sample(0.1, 2018) # test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instancing Machine Learning Pipelines, Indexer, Enconder, Scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- plantURI: string (nullable = true)\n",
      " |-- 1_avg(convex_hull_area): double (nullable = true)\n",
      " |-- 1_avg(object_sum_area): double (nullable = true)\n",
      " |-- 1_avg(width): double (nullable = true)\n",
      " |-- 1_avg(height_over_pot): double (nullable = true)\n",
      " |-- 2_avg(convex_hull_area): double (nullable = true)\n",
      " |-- 2_avg(object_sum_area): double (nullable = true)\n",
      " |-- 2_avg(width): double (nullable = true)\n",
      " |-- 2_avg(height_over_pot): double (nullable = true)\n",
      " |-- 3_avg(convex_hull_area): double (nullable = true)\n",
      " |-- 3_avg(object_sum_area): double (nullable = true)\n",
      " |-- 3_avg(width): double (nullable = true)\n",
      " |-- 3_avg(height_over_pot): double (nullable = true)\n",
      " |-- 4_avg(convex_hull_area): double (nullable = true)\n",
      " |-- 4_avg(object_sum_area): double (nullable = true)\n",
      " |-- 4_avg(width): double (nullable = true)\n",
      " |-- 4_avg(height_over_pot): double (nullable = true)\n",
      " |-- Treatment: string (nullable = true)\n",
      " |-- biomass: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, MinMaxScaler, StandardScaler\n",
    "from pyspark.ml.feature import PCA as PCAml\n",
    "from pyspark.sql.functions import log, col\n",
    "\n",
    "print(df_dataset.printSchema())\n",
    "df_dataset_final= df_dataset.drop(\"plantURI\")#,\"1_avg(object_sum_area)\", \"1_avg(object_sum_area)\")\n",
    "\n",
    "target_y = 'biomass'\n",
    "target_renamed_original= \"label\"\n",
    "\n",
    "df_dataset_final=df_dataset_final.withColumnRenamed( target_y , target_renamed_original)\n",
    "\n",
    "df_dataset_final=df_dataset_final.na.drop()\n",
    "\n",
    "df_dataset_final.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "#df_dataset_final.cache()  # keep in memory for performance\n",
    "\n",
    "# df_pd_sampling[df_pd_sampling['Treatment']=='WW'][target_renamed_original].hist()\n",
    "# df_pd_sampling[df_pd_sampling['Treatment']=='WD'][target_renamed_original].hist(alpha=0.5)\n",
    "# #df_dataset_final=df_dataset_final.drop('biomass')\n",
    "# plt.savefig(FOLDER +'/target_original_histogram.png')\n",
    "\n",
    "def enconder_indexer_stages(df_interno):\n",
    "    \n",
    "    array_labels= [ target_renamed_original ]\n",
    "    #df_interno.cache() # keep in memory for performance\n",
    "    \n",
    "    df_interno = df_interno\n",
    "\n",
    "    numeric_features = [t[0] for t in df_interno.dtypes if t[1] == 'double' and t[0] not in array_labels]\n",
    "\n",
    "    categorical_features = [t[0] for t in df_interno.dtypes if t[1] == 'string' and t[0] not in array_labels]\n",
    "    \n",
    "    print(numeric_features)\n",
    "    print(categorical_features)\n",
    "    stages = []\n",
    "    for categoricalCol in categorical_features:\n",
    "        stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "        stages += [stringIndexer, encoder]\n",
    "\n",
    "\n",
    "    assemblerInputs = [c + \"classVec\" for c in categorical_features] + numeric_features\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    stages+=[assembler]\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "    stages+=[scaler]\n",
    "    \n",
    "    if FEATURE_SELECTION_METHOD == FEATURE_SELECTION_PCA:\n",
    "        pca = PCAml(k=int(PCA_OPTION), inputCol='features_scaled', outputCol=\"features_pca\")\n",
    "        stages+=[pca]\n",
    "    \n",
    "    return stages\n",
    "\n",
    "\n",
    "\n",
    "# df_pd_sampling[df_pd_sampling['Treatment']=='WW'][target_renamed_log].hist()\n",
    "# df_pd_sampling[df_pd_sampling['Treatment']=='WD'][target_renamed_log].hist(alpha=0.5)\n",
    "\n",
    "# plt.savefig(FOLDER +'/target_log_histogram.png')\n",
    "\n",
    "#df_dataset_final.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Executors for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "def cross_validation(pipeline, param_grid, evaluator):\n",
    "    \n",
    "    K_FOLDS=5\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=param_grid,\n",
    "                              numFolds=K_FOLDS,\n",
    "                              evaluator= evaluator\n",
    "                             ) \n",
    "    return crossval\n",
    "# training dataset \n",
    "def split_dataframe(df_interno, training_rate= 0.8):\n",
    "    \n",
    "    TRAINING=training_rate\n",
    "    TEST= 1.0 - training_rate \n",
    "    train, test = df_interno.randomSplit([TRAINING, TEST], seed = 2018)\n",
    "    #print(\"Training Dataset Count: \" + str(train.count()))\n",
    "    #print(\"Test Dataset Count: \" + str(test.count()))\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def ridge_regression_executor(df_interno):\n",
    "    \n",
    "    print(\"ridge regression /////////\")\n",
    "    lr = LinearRegression( maxIter=100, featuresCol=FEATURE_ML, labelCol=LABEL_ML , solver=\"l-bfgs\")\n",
    "                          #tol=1e-6, fitIntercept=True, standardization=True, solver=\"auto\",weightCol=None, aggregationDepth=2)\n",
    "    print(lr.solver)\n",
    "    stages_lr= enconder_indexer_stages(df_interno)\n",
    "    stages_lr = stages_lr+[lr] # append linear regression models as steps\n",
    "\n",
    "    pipeline_lr = Pipeline(stages = stages_lr)\n",
    "    \n",
    "    # ridge regression 0.0, lasso= 1.0\n",
    "    param_lr = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.5,0.9]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0]) \\\n",
    "        .build()\n",
    "    \n",
    "    \n",
    "    model_crossval = cross_validation(pipeline_lr, param_lr, RegressionEvaluator(metricName=METRIC_PERFORMANCE_ML))\n",
    "    \n",
    "    model_list_lr = model_crossval.fit(df_interno)\n",
    "    \n",
    "    return model_list_lr\n",
    "\n",
    "def linear_regression_executor(df_interno):\n",
    "    \n",
    "    print(\"linear regression /////////\")\n",
    "    lr = LinearRegression( maxIter=100, featuresCol=FEATURE_ML, labelCol=LABEL_ML, solver=\"l-bfgs\")\n",
    "    print(lr.solver)\n",
    "    stages_lr= enconder_indexer_stages(df_interno)\n",
    "    stages_lr = stages_lr+[lr] # append linear regression models as steps\n",
    "\n",
    "    pipeline_lr = Pipeline(stages = stages_lr)\n",
    "    \n",
    "    param_lr = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.0])\\\n",
    "        .build()\n",
    "    model_crossval = cross_validation(pipeline_lr, param_lr, RegressionEvaluator(metricName=METRIC_PERFORMANCE_ML))\n",
    "    \n",
    "    model_list_lr = model_crossval.fit(df_interno)\n",
    "    \n",
    "    return model_list_lr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Executors for Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.functions import mean, col, lit\n",
    "\n",
    "def random_forest_executor(df_interno):\n",
    "    \n",
    "\n",
    "    rf = RandomForestRegressor(featuresCol = FEATURE_ML, labelCol = LABEL_ML , maxDepth=6 ,seed=42)\n",
    "    \n",
    "    stages_rf= enconder_indexer_stages(df_interno)\n",
    "    stages_rf = stages_rf+[rf] # append linear regression models as steps\n",
    "\n",
    "    pipeline_rf = Pipeline(stages = stages_rf)\n",
    "    \n",
    "    param_rf = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [10,100,1000]) \\\n",
    "        .build()\n",
    "    \n",
    "    \n",
    "    model_crossval = cross_validation(pipeline_rf, param_rf, RegressionEvaluator(metricName=METRIC_PERFORMANCE_ML))\n",
    "    \n",
    "    model_list_rf = model_crossval.fit(df_interno)\n",
    "    \n",
    "    return model_list_rf\n",
    "\n",
    "class DummieModel:\n",
    "    \n",
    "    def __init__(self,df_interno):\n",
    "        self.mean_label = df_interno.select( mean(col(LABEL_ML)).alias('mean')).collect()[0]['mean']\n",
    "        print(self.mean_label)\n",
    "    \n",
    "    def transform(self,df_interno):\n",
    "\n",
    "        return  df_interno.withColumn('prediction', lit(self.mean_label))\n",
    "    def save(self, folder):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for Organizing and Evaluating Models Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from itertools import cycle\n",
    "cycol = cycle('bgrcmk')\n",
    "\n",
    "def calc_rmse(label_col,df_predictions):\n",
    "    rf_evaluator = RegressionEvaluator(\n",
    "    labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = rf_evaluator.evaluate(df_predictions)\n",
    "    return rmse\n",
    "\n",
    "def calc_rsquare(label_col, df_predictions):\n",
    "    rf_evaluator = RegressionEvaluator(\n",
    "    labelCol=label_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    r2 = rf_evaluator.evaluate(df_predictions)\n",
    "    return r2\n",
    "    \n",
    "def rf_output(model_crossv_rf):\n",
    "    result={'model':\"rf\"}\n",
    "    result['r2_crossval']=str(model_crossv_rf.avgMetrics)\n",
    "    result['importance_list']=[]\n",
    "    #importance= model_crossv_rf.bestModel.stages[-1].featureImportances \n",
    "    #input_cols_rf = model_crossv_rf.bestModel.stages[-2].getInputCols()\n",
    "    #for x in range(len(input_cols_rf)):\n",
    "        #result['importance_list'].append({'feature':input_cols_rf[x],'importance':importance[x]})\n",
    "    return result\n",
    "\n",
    "def lr_output(model_crossv_lr):\n",
    "    result={'model':\"lr\"}\n",
    "    result['r2_crossval']=str(model_crossv_lr.avgMetrics)\n",
    "    lr_model= model_crossv_lr.bestModel.stages[-1] # extract linear regression model\n",
    "#     lr_summary = lr_model.summary\n",
    "#     result['rmse']=lr_summary.rootMeanSquaredError\n",
    "#     result['r2_summary']=lr_summary.r2\n",
    "    result['coefficients']= str(lr_model.coefficients)\n",
    "    result['intercept']= str(lr_model.intercept)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the scenery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression /////////\n",
      "LinearRegression_49a1ae06a4a0674c43f6__solver\n",
      "['1_avg(convex_hull_area)', '1_avg(object_sum_area)', '1_avg(width)', '1_avg(height_over_pot)', '2_avg(convex_hull_area)', '2_avg(object_sum_area)', '2_avg(width)', '2_avg(height_over_pot)', '3_avg(convex_hull_area)', '3_avg(object_sum_area)', '3_avg(width)', '3_avg(height_over_pot)', '4_avg(convex_hull_area)', '4_avg(object_sum_area)', '4_avg(width)', '4_avg(height_over_pot)']\n",
      "['Treatment']\n",
      "Calculating errors\n",
      "ridge regression /////////\n",
      "LinearRegression_4eae89c2e7c7317e76fa__solver\n",
      "['1_avg(convex_hull_area)', '1_avg(object_sum_area)', '1_avg(width)', '1_avg(height_over_pot)', '2_avg(convex_hull_area)', '2_avg(object_sum_area)', '2_avg(width)', '2_avg(height_over_pot)', '3_avg(convex_hull_area)', '3_avg(object_sum_area)', '3_avg(width)', '3_avg(height_over_pot)', '4_avg(convex_hull_area)', '4_avg(object_sum_area)', '4_avg(width)', '4_avg(height_over_pot)']\n",
      "['Treatment']\n",
      "Calculating errors\n",
      "['1_avg(convex_hull_area)', '1_avg(object_sum_area)', '1_avg(width)', '1_avg(height_over_pot)', '2_avg(convex_hull_area)', '2_avg(object_sum_area)', '2_avg(width)', '2_avg(height_over_pot)', '3_avg(convex_hull_area)', '3_avg(object_sum_area)', '3_avg(width)', '3_avg(height_over_pot)', '4_avg(convex_hull_area)', '4_avg(object_sum_area)', '4_avg(width)', '4_avg(height_over_pot)']\n",
      "['Treatment']\n",
      "Calculating errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if FEATURE_SELECTION_METHOD== FEATURE_SELECTION_PCA  or FEATURE_SELECTION_METHOD== FEATURE_SELECTION_ALL:\n",
    "    sceneries=[\n",
    "        df_dataset_final.columns\n",
    "    ]\n",
    "elif FEATURE_SELECTION_METHOD== FEATURE_SELECTION_NAIVE:\n",
    "    sceneries=[\n",
    "       selected_items_columns + [LABEL_ML]\n",
    "    ]\n",
    "    \n",
    "def run_sceneries(df_training, df_test):\n",
    "    \n",
    "    \n",
    "    lr_model_config = {'label':'lr', \"executor\":linear_regression_executor, \"output\":lr_output}\n",
    "    lr_model_ridge_config = {'label':'lr_ridge', \"executor\":ridge_regression_executor, \"output\":lr_output}\n",
    "    rf_model_ridge_config = {'label':'rf', \"executor\":random_forest_executor, \"output\":rf_output}\n",
    "    \n",
    "    #baseline_model_config = {'label':'baseline', \"executor\":DummieModel, \"output\":rf_output}\n",
    "    \n",
    "#     if MODEL_TYPE == \"lr\":\n",
    "#         models = [baseline_model_config, lr_model_config, lr_model_ridge_config ]\n",
    "#     elif MODEL_TYPE == \"rf\":         \n",
    "#         models = [ rf_model_ridge_config]\n",
    "    models = [ lr_model_config, lr_model_ridge_config , rf_model_ridge_config]\n",
    "    \n",
    "    models_final_result = {}\n",
    "    \n",
    "    plot_sampling_test = []\n",
    "        \n",
    "    SAMPLING_SIZE= 200  \n",
    "    \n",
    "    for local_model in models:\n",
    "        result = {'start':str(dt.datetime.now())}\n",
    "        model_result = local_model[\"executor\"](df_training)\n",
    "        \n",
    "        \n",
    "        result_model_training = model_result.transform(df_training)\n",
    "        result_model_test = model_result.transform(df_test)\n",
    "        \n",
    "\n",
    "        df_pd_result=result_model_test.select(LABEL_ML,\"prediction\").limit(SAMPLING_SIZE).toPandas()\n",
    "        df_pd_result.to_csv(FOLDER+'/%s_test_sampling.csv'%(local_model['label']), index = False, header=True)\n",
    "        \n",
    "        print(\"Calculating errors\")\n",
    "        result['r2_training']= calc_rsquare(LABEL_ML,result_model_training)\n",
    "        result['rmse_training']= calc_rmse(LABEL_ML,result_model_training)\n",
    "        result['r2_test']= calc_rsquare(LABEL_ML,result_model_test)\n",
    "        result['rmse_test']= calc_rmse(LABEL_ML,result_model_test)\n",
    "        \n",
    "        result['end']= str(dt.datetime.now())\n",
    "        \n",
    "        explore_result = local_model[\"output\"](model_result)\n",
    "        result[\"model_output\"]= explore_result\n",
    "        \n",
    "        models_final_result[local_model['label']]= result\n",
    "        \n",
    "        \n",
    "    #plot_errors_models_label(plot_sampling_test)\n",
    "    \n",
    "    \n",
    "    return models_final_result \n",
    "\n",
    "def run_all():\n",
    "    for sc in sceneries:\n",
    "\n",
    "        info_run=run_sceneries(training.select(sc), test.select(sc) )\n",
    "\n",
    "        result_local={'scenery':sc,'models_info':info_run}\n",
    "\n",
    "        result_notebook['sceneries'].append(result_local)\n",
    "\n",
    "        \n",
    "result_notebook['sceneries']=[]\n",
    "training, test = split_dataframe(df_dataset_final)\n",
    "\n",
    "run_all()\n",
    "\n",
    "result_notebook['end']=str(dt.datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results in a JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"angle\": \"AVG\",\n",
      "    \"columns_images\": [\n",
      "        \"plantURI\",\n",
      "        \"dayOfYear\",\n",
      "        \"Silk_area\",\n",
      "        \"convex_hull_area\",\n",
      "        \"convex_hull_perimeter\",\n",
      "        \"height\",\n",
      "        \"height_over_pot\",\n",
      "        \"height_under_pot\",\n",
      "        \"number_of_objects\",\n",
      "        \"object_sum_area\",\n",
      "        \"width\"\n",
      "    ],\n",
      "    \"end\": \"2020-03-25 21:51:22.426236\",\n",
      "    \"folder\": \"results/angle_AVG/slots_4/all\",\n",
      "    \"sceneries\": [\n",
      "        {\n",
      "            \"models_info\": {\n",
      "                \"lr\": {\n",
      "                    \"end\": \"2020-03-25 21:38:34.978736\",\n",
      "                    \"model_output\": {\n",
      "                        \"coefficients\": \"[1.3403921777125678,-29.107091384057327,-53.10424339749276,9.02060814977152,77.60437764565454,39.9518928097993,33.5917276013052,-38.73530020633246,-74.84994393989513,34.21081861907875,-59.57724864440271,8.53232077168436,30.07704573165963,-102.42163752743275,175.50810343438235,21.438228570111814,11.557652648847508]\",\n",
      "                        \"intercept\": \"-8.083891727144646\",\n",
      "                        \"model\": \"lr\",\n",
      "                        \"r2_crossval\": \"[73.52189476381025]\"\n",
      "                    },\n",
      "                    \"r2_test\": 0.6179227058758083,\n",
      "                    \"r2_training\": 0.7887760260558504,\n",
      "                    \"rmse_test\": 73.77941094755279,\n",
      "                    \"rmse_training\": 54.404337334860934,\n",
      "                    \"start\": \"2020-03-25 21:34:34.258872\"\n",
      "                },\n",
      "                \"lr_ridge\": {\n",
      "                    \"end\": \"2020-03-25 21:43:36.026497\",\n",
      "                    \"model_output\": {\n",
      "                        \"coefficients\": \"[4.117723349834899,-26.15310137470254,-43.729458614873735,6.887230177266547,65.81529378061603,32.326762649565666,14.26655649785777,-28.458767739158635,-58.71101482916656,15.21856459442583,-23.31065782085369,9.44204817738581,25.368575680505455,-56.63022860179695,142.61201061142182,9.427007873593821,-2.487906470503155]\",\n",
      "                        \"intercept\": \"36.34143467599358\",\n",
      "                        \"model\": \"lr\",\n",
      "                        \"r2_crossval\": \"[71.4398106363121, 70.7838308290434]\"\n",
      "                    },\n",
      "                    \"r2_test\": 0.6265802568577741,\n",
      "                    \"r2_training\": 0.7825004346971989,\n",
      "                    \"rmse_test\": 72.93873160671161,\n",
      "                    \"rmse_training\": 55.20661471359024,\n",
      "                    \"start\": \"2020-03-25 21:38:34.989672\"\n",
      "                },\n",
      "                \"rf\": {\n",
      "                    \"end\": \"2020-03-25 21:51:22.425239\",\n",
      "                    \"model_output\": {\n",
      "                        \"importance_list\": [],\n",
      "                        \"model\": \"rf\",\n",
      "                        \"r2_crossval\": \"[83.47686942555403, 78.00929788703205, 76.35727596811572]\"\n",
      "                    },\n",
      "                    \"r2_test\": 0.4414317622003272,\n",
      "                    \"r2_training\": 0.9286881497237416,\n",
      "                    \"rmse_test\": 89.20674377687766,\n",
      "                    \"rmse_training\": 31.61133030343154,\n",
      "                    \"start\": \"2020-03-25 21:43:36.029489\"\n",
      "                }\n",
      "            },\n",
      "            \"scenery\": [\n",
      "                \"1_avg(convex_hull_area)\",\n",
      "                \"1_avg(object_sum_area)\",\n",
      "                \"1_avg(width)\",\n",
      "                \"1_avg(height_over_pot)\",\n",
      "                \"2_avg(convex_hull_area)\",\n",
      "                \"2_avg(object_sum_area)\",\n",
      "                \"2_avg(width)\",\n",
      "                \"2_avg(height_over_pot)\",\n",
      "                \"3_avg(convex_hull_area)\",\n",
      "                \"3_avg(object_sum_area)\",\n",
      "                \"3_avg(width)\",\n",
      "                \"3_avg(height_over_pot)\",\n",
      "                \"4_avg(convex_hull_area)\",\n",
      "                \"4_avg(object_sum_area)\",\n",
      "                \"4_avg(width)\",\n",
      "                \"4_avg(height_over_pot)\",\n",
      "                \"Treatment\",\n",
      "                \"label\"\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"slots\": {\n",
      "        \"split\": 8,\n",
      "        \"until\": 30\n",
      "    },\n",
      "    \"start\": \"2020-03-25 21:31:57.328874\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#result_notebook['step_wise_naive']=str(result_notebook['step_wise_naive'])\n",
    "print(json.dumps(result_notebook, indent=4,  sort_keys=True))\n",
    "\n",
    "with open(FOLDER+'/models_info_%s.json'%(MODEL_TYPE), 'w') as outfile:\n",
    "    json.dump(result_notebook, outfile, indent=4,  sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_avg(convex_hull_area)',\n",
       " '1_avg(object_sum_area)',\n",
       " '1_avg(width)',\n",
       " '1_avg(height_over_pot)',\n",
       " '2_avg(convex_hull_area)',\n",
       " '2_avg(object_sum_area)',\n",
       " '2_avg(width)',\n",
       " '2_avg(height_over_pot)',\n",
       " '3_avg(convex_hull_area)',\n",
       " '3_avg(object_sum_area)',\n",
       " '3_avg(width)',\n",
       " '3_avg(height_over_pot)',\n",
       " '4_avg(convex_hull_area)',\n",
       " '4_avg(object_sum_area)',\n",
       " '4_avg(width)',\n",
       " '4_avg(height_over_pot)',\n",
       " 'Treatment',\n",
       " 'label']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result_local\n",
    "df_dataset_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_final.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
