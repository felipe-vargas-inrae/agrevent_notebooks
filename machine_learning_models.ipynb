{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-driven approach to predict Biomass\n",
    "\n",
    "For understanding this notebook is neccesary to check the explore_imagen_analysis notebook. Here, machine learning models implemented in Pyspark are used to predict the final biomass.\n",
    "\n",
    "## Loading PySpark session\n",
    "\n",
    "Next lines represent the code to start spark session into the machine. Additionally, some functions are implemented to extract information from MongoDB and create PySpark Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "#%matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## init the spark session and impose the MongoDB connector\n",
    "MONGO_URI=\"mongodb://localhost:27017/iot_db\" \n",
    "my_spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .master('local[*]')\\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.3.2\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", MONGO_URI+\".phis_experiments\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def get_collection_mongodb(collection, pipeline=None) :\n",
    "    '''Get one collection from MongoDB database, the pipeline parameter is optional'''\n",
    "    options = my_spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "                                            .option(\"database\",\"iot_db\")\\\n",
    "                                            .option(\"collection\", collection)                                           \n",
    "    if pipeline: \n",
    "        options.option(\"pipeline\", pipeline)\n",
    "    return options.load()\n",
    "\n",
    "def pipeline_angle(angle):\n",
    "    '''Pipeline send to MongoDB to extract the angle information'''\n",
    "    return  \"{'$match': {'angle':'%s' }}\"%(angle)\n",
    "\n",
    "   \n",
    "#stats functions\n",
    "def cal_correlation(df):\n",
    "    '''stats function to calculate the correlation between the columns, asumming that all are numeric'''\n",
    "    col_names = df.columns\n",
    "    features = df.rdd.map(lambda row: row[0:])\n",
    "\n",
    "    corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "    corr_df = pd.DataFrame(corr_mat)\n",
    "    corr_df.index, corr_df.columns = col_names, col_names\n",
    "    return corr_df\n",
    "\n",
    "def join_dataframes(df1,df2, f1_column, f2_column): \n",
    "    '''Apply join function to two spark dataframes'''\n",
    "    ta = df1.alias('ta')\n",
    "    tb = df2.alias('tb')\n",
    "    \n",
    "    if f1_column == f2_column: ## avoid repeat column in join result\n",
    "        df_join = ta.join(tb,[f1_column])\n",
    "    else:   \n",
    "        df_join = ta.join(tb, ta[f1_column] == tb[f2_column])\n",
    "    return df_join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring a scenery to run this notebook\n",
    "\n",
    "The scenery is a set of configurations aim to define how the notebook will predict the biomass. For example, the angle to explore, the feature extraction technique to use, the slots number\n",
    "\n",
    "Line by line will be explain this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After running the data exploration notebook some plants contain many null rows > 23, in this list are all these plants. \n",
    "# potenicial plants that has problems during the growth season, it might be because camera errors, algortihms erros, human erros\n",
    "potencial_nulls= [\n",
    "    'http://www.phenome-fppn.fr/m3p/arch/2017/c17000795'\n",
    ",'http://www.phenome-fppn.fr/m3p/arch/2017/c17000469'\n",
    ",'http://www.phenome-fppn.fr/m3p/arch/2017/c17001536']\n",
    "\n",
    "\n",
    "## imagen analysis can be disaggregated by the camera angle \n",
    "# all the angles takes in phenoarch\n",
    "# 0 is removed because doesn't have heigth over pot \n",
    "ANGLES= [\"30\",\"60\",\"90\",\"120\",\"150\",\"180\",\"210\",\"240\",\"270\",\"300\",\"330\",\"360\"] \n",
    "\n",
    "# ANGLE is the constant that is used in the next lines to indentify the camera ANGLE, some element of ANGLES array\n",
    "ANGLE =  ANGLES[9] #240      \n",
    "\n",
    "LABEL_ML= \"label\" # column from dataset that indicates the output variable to predict, label=biomass\n",
    "METRIC_PERFORMANCE_ML=\"rmse\" # the metrict to select models into the crossvalition iterations\n",
    "ENTITY_URI_COL= \"plantURI\"# the entity that joins biomass Dataframe with imagenAnalysis Dataframe \n",
    "DATE_COL = \"dayOfYear\" # the column that indentifies the date value\n",
    "\n",
    "TIME_SERIES_COLLECTION= \"phis_imagen_analysis_explicit_angle\" # timeseries imagen analysis collection name\n",
    "SUMMARY_DATA = \"phis_biomass\" # biomass collection name\n",
    "\n",
    "# some features selection options, all is to use all the features, pca is to use pca technique, \n",
    "# naive is a two steps implementation, (1) get the most relevant features for biomass, (2) filter the correlated with the features selected\n",
    "FEATURE_SELECTION_ALL = \"all\" \n",
    "FEATURE_SELECTION_PCA = \"pca\"\n",
    "FEATURE_SELECTION_NAIVE = \"naive\"\n",
    "FEATURE_SELECTION_METHOD= FEATURE_SELECTION_ALL # configure the feature option in the notebook\n",
    "PCA_OPTION =\"4\" #[\"2\",\"3\",\"4\"]  the pca number of components\n",
    "\n",
    "if FEATURE_SELECTION_METHOD==FEATURE_SELECTION_PCA: # when PCA is used as method some columns names change in the pipeline\n",
    "    FEATURE_ML = 'features_pca'\n",
    "    LAST_FOLDER = FEATURE_SELECTION_PCA+\"_\"+PCA_OPTION  \n",
    "else :\n",
    "    FEATURE_ML = 'features_scaled'\n",
    "    LAST_FOLDER = FEATURE_SELECTION_METHOD # False when there is not used PCA\n",
    "\n",
    "## how many slots to use  \n",
    "SLOTS= \"4\"\n",
    "until_ndays=30 # get first n days from time series data\n",
    "split_ndays=8 # groups size of ndays days \n",
    "groups= [1,2,3,4] # groups label, suitable for performance improvenment \n",
    "\n",
    "ACTIVATE_SAMPLING = False # this line is to select 10% of dataset when the notebook is tested\n",
    "\n",
    "MODEL_TYPE = \"lr\" # first  run linear regression models to avoid kernel errors then run rf models\n",
    "\n",
    "\n",
    "FOLDER = \"results/angle_%s/slots_%s/%s\"%(ANGLE,SLOTS,LAST_FOLDER) # a folder to store the scenery \n",
    "isDirectory = os.path.isdir(FOLDER)\n",
    "print(isDirectory,FOLDER) ## validate that the folder exists\n",
    "\n",
    "# result_notebook is a dictionary to save the final result in a File\n",
    "result_notebook= {'angle':ANGLE}\n",
    "result_notebook['folder']= FOLDER\n",
    "result_notebook['start']=str(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining time-series data from MongoDB\n",
    "<i>Loadding collection imagenanalysesangle_aggregated_by_day</i>\n",
    "\n",
    "in this poin is used the function ``get_collection_mongodb``, which allows to get a MongoDB collection, the loaded collection is mapped to a Dataframe. Following, the relevant features are projected. Finally variablecode values are reshape to columns by invoking the ``pivot`` function. \n",
    "\n",
    "Since variable code is an arbitrary list of variable names, this pivot process gives flexibility to the pre-processing and allows to add other predictors, for example, a new information about the plant. The schema is thought to facilitate this called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hour to day transformacion\n",
    "def get_by_angle(angle):\n",
    "    \n",
    "    \n",
    "    pipeline = pipeline_angle(angle)\n",
    "    df_images_anaysis = get_collection_mongodb( TIME_SERIES_COLLECTION ,pipeline)\n",
    "    df_images_anaysis = df_images_anaysis.filter(~df_images_anaysis[ENTITY_URI_COL].isin(potencial_nulls))\n",
    "    df_images_anaysis = df_images_anaysis.select(ENTITY_URI_COL, DATE_COL, \"variableCode\", \"value\")\n",
    "    df_images_anaysis = df_images_anaysis.groupBy(ENTITY_URI_COL, DATE_COL).pivot(\"variableCode\").mean(\"value\")\n",
    "    #df_images_anaysis = df_images_anaysis.orderBy(ENTITY_URI_COL,DATE_COL)\n",
    "    \n",
    "    result_notebook['columns_images']=df_images_anaysis.columns\n",
    "    #df_images_anaysis.show(200)\n",
    "    return df_images_anaysis\n",
    "    \n",
    "df_select_angle=get_by_angle(ANGLE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing time-series data \n",
    "\n",
    "<i>Apply pyspark transformations/actions to aggregate time-series data</i>\n",
    "\n",
    "Create a window lead by plantURI and dayOfYear fields, this window represents a plant growth season,\n",
    "the row_number is for numbering  the days inside the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import *\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "# porcion del dataset donde se realiza la secuencia, si hay dos ventanas se repetira la posicion 1 para cada ventana\n",
    "windowSpec = Window.partitionBy(ENTITY_URI_COL).orderBy(DATE_COL)\n",
    "# definción de transformación \n",
    "df_grp_img_analysis_row_number= df_select_angle.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "\n",
    "df_grp_img_analysis_row_number_biomass=df_grp_img_analysis_row_number.select(ENTITY_URI_COL,'row_number',\n",
    "                                              'convex_hull_area','height_over_pot','width','object_sum_area')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the number of slots and filter time-series data\n",
    "\n",
    "In this process, having the number of slots, a new column is defined to identify the group for each row,\n",
    "this column will be used for grouping. Second line is about to filter the time-series data, using\n",
    "the first ``until_days``. Finally, using the ```pivot``` function time-series data is aggregated, transforming\n",
    "one variable into n variables equivalent to the number of slots. The result is a dataframe with the same granularity of  summary data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_notebook['slots']={'until':until_ndays,'split':split_ndays}\n",
    "df_grp_img_analysis_row_number_biomass= df_grp_img_analysis_row_number_biomass\\\n",
    "                                                                    .withColumn(\"split_ndays_group\",\\\n",
    "                                                                      F.ceil(F.col(\"row_number\") /split_ndays))\n",
    "\n",
    "## filter to get first days \n",
    "df_grp_img_analysis_row_number_biomass_first_ndays=df_grp_img_analysis_row_number_biomass[F.col(\"row_number\")<=until_ndays]\n",
    "\n",
    "# applied pivot exclusively to certainty columns\n",
    "METHOD_AGG= 'avg'\n",
    "exprsPivoted = {\"convex_hull_area\":METHOD_AGG,'height_over_pot':METHOD_AGG,'width':METHOD_AGG ,\"object_sum_area\":METHOD_AGG}\n",
    "\n",
    "df_grp_img_analysis_row_number_biomass_split= df_grp_img_analysis_row_number_biomass_first_ndays\\\n",
    "                            .groupBy('plantURI').pivot('split_ndays_group',groups).agg(exprsPivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining biomass and time-series imagen analysis data\n",
    "\n",
    "In this process is created the biomass Dataframe, again invoking the ``get_collection_mongodb``function, some features are projected. In this way, a dataset is created by joining biomass Dataframe with time-series imagen analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_biomasa= get_collection_mongodb(SUMMARY_DATA)\n",
    "excel_biomasa= excel_biomasa.select('plantURI','Treatment','Biomass(gramos_pesofresco)')\n",
    "df_dataset = join_dataframes(df_grp_img_analysis_row_number_biomass_split,excel_biomasa,ENTITY_URI_COL,ENTITY_URI_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the dataset\n",
    "\n",
    "First will be deleted all the records where biomass field is null, the biomass field is casting to double, in order to manipulate as a number, which is required in regression problems. Finally, the flag ``ACTIVATE_SAMPLING`` is checked to know if the dataset must be reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows with no biomass value\n",
    "df_dataset = df_dataset.na.drop(subset=['Biomass(gramos_pesofresco)'])\n",
    "\n",
    "df_dataset=df_dataset.withColumn(\"biomass\", df_dataset[\"Biomass(gramos_pesofresco)\"].cast(\"double\"))\n",
    "\n",
    "df_dataset= df_dataset.drop('Biomass(gramos_pesofresco)')\n",
    "\n",
    "if ACTIVATE_SAMPLING:\n",
    "    df_dataset= df_dataset.sample(0.1, 2018) # test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instancing Machine Learning Pipelines, Indexer, Enconder, Scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, MinMaxScaler, StandardScaler\n",
    "from pyspark.ml.feature import PCA as PCAml\n",
    "from pyspark.sql.functions import log, col\n",
    "\n",
    "print(df_dataset.printSchema())\n",
    "df_dataset_final= df_dataset.drop(\"plantURI\")#,\"1_avg(object_sum_area)\", \"1_avg(object_sum_area)\")\n",
    "\n",
    "target_y = 'biomass'\n",
    "target_renamed_original= \"label\"\n",
    "\n",
    "df_dataset_final=df_dataset_final.withColumnRenamed( target_y , target_renamed_original)\n",
    "\n",
    "df_dataset_final=df_dataset_final.na.drop()\n",
    "\n",
    "df_dataset_final.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "#df_dataset_final.cache()  # keep in memory for performance\n",
    "\n",
    "# df_pd_sampling[df_pd_sampling['Treatment']=='WW'][target_renamed_original].hist()\n",
    "# df_pd_sampling[df_pd_sampling['Treatment']=='WD'][target_renamed_original].hist(alpha=0.5)\n",
    "# #df_dataset_final=df_dataset_final.drop('biomass')\n",
    "# plt.savefig(FOLDER +'/target_original_histogram.png')\n",
    "\n",
    "def enconder_indexer_stages(df_interno):\n",
    "    \n",
    "    array_labels= [ target_renamed_original ]\n",
    "    #df_interno.cache() # keep in memory for performance\n",
    "    \n",
    "    df_interno = df_interno\n",
    "\n",
    "    numeric_features = [t[0] for t in df_interno.dtypes if t[1] == 'double' and t[0] not in array_labels]\n",
    "\n",
    "    categorical_features = [t[0] for t in df_interno.dtypes if t[1] == 'string' and t[0] not in array_labels]\n",
    "    \n",
    "    print(numeric_features)\n",
    "    print(categorical_features)\n",
    "    stages = []\n",
    "    for categoricalCol in categorical_features:\n",
    "        stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "        stages += [stringIndexer, encoder]\n",
    "\n",
    "\n",
    "    assemblerInputs = [c + \"classVec\" for c in categorical_features] + numeric_features\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    stages+=[assembler]\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "    stages+=[scaler]\n",
    "    \n",
    "    if FEATURE_SELECTION_METHOD == FEATURE_SELECTION_PCA:\n",
    "        pca = PCAml(k=int(PCA_OPTION), inputCol='features_scaled', outputCol=\"features_pca\")\n",
    "        stages+=[pca]\n",
    "    \n",
    "    return stages\n",
    "\n",
    "\n",
    "\n",
    "# df_pd_sampling[df_pd_sampling['Treatment']=='WW'][target_renamed_log].hist()\n",
    "# df_pd_sampling[df_pd_sampling['Treatment']=='WD'][target_renamed_log].hist(alpha=0.5)\n",
    "\n",
    "# plt.savefig(FOLDER +'/target_log_histogram.png')\n",
    "\n",
    "#df_dataset_final.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Executors for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "def cross_validation(pipeline, param_grid, evaluator):\n",
    "    \n",
    "    K_FOLDS=5\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=param_grid,\n",
    "                              numFolds=K_FOLDS,\n",
    "                              evaluator= evaluator\n",
    "                             ) \n",
    "    return crossval\n",
    "# training dataset \n",
    "def split_dataframe(df_interno, training_rate= 0.8):\n",
    "    \n",
    "    TRAINING=training_rate\n",
    "    TEST= 1.0 - training_rate \n",
    "    train, test = df_interno.randomSplit([TRAINING, TEST], seed = 2018)\n",
    "    #print(\"Training Dataset Count: \" + str(train.count()))\n",
    "    #print(\"Test Dataset Count: \" + str(test.count()))\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def ridge_regression_executor(df_interno):\n",
    "    \n",
    "    print(\"ridge regression /////////\")\n",
    "    lr = LinearRegression( maxIter=100, featuresCol=FEATURE_ML, labelCol=LABEL_ML , solver=\"l-bfgs\")\n",
    "                          #tol=1e-6, fitIntercept=True, standardization=True, solver=\"auto\",weightCol=None, aggregationDepth=2)\n",
    "    print(lr.solver)\n",
    "    stages_lr= enconder_indexer_stages(df_interno)\n",
    "    stages_lr = stages_lr+[lr] # append linear regression models as steps\n",
    "\n",
    "    pipeline_lr = Pipeline(stages = stages_lr)\n",
    "    \n",
    "    # ridge regression 0.0, lasso= 1.0\n",
    "    param_lr = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.5,0.9]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0]) \\\n",
    "        .build()\n",
    "    \n",
    "    \n",
    "    model_crossval = cross_validation(pipeline_lr, param_lr, RegressionEvaluator(metricName=METRIC_PERFORMANCE_ML))\n",
    "    \n",
    "    model_list_lr = model_crossval.fit(df_interno)\n",
    "    \n",
    "    return model_list_lr\n",
    "\n",
    "def linear_regression_executor(df_interno):\n",
    "    \n",
    "    print(\"linear regression /////////\")\n",
    "    lr = LinearRegression( maxIter=100, featuresCol=FEATURE_ML, labelCol=LABEL_ML, solver=\"l-bfgs\")\n",
    "    print(lr.solver)\n",
    "    stages_lr= enconder_indexer_stages(df_interno)\n",
    "    stages_lr = stages_lr+[lr] # append linear regression models as steps\n",
    "\n",
    "    pipeline_lr = Pipeline(stages = stages_lr)\n",
    "    \n",
    "    param_lr = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.0])\\\n",
    "#         .addGrid(lr.elasticNetParam, [0.0])\\\n",
    "        .build()\n",
    "    model_crossval = cross_validation(pipeline_lr, param_lr, RegressionEvaluator(metricName=METRIC_PERFORMANCE_ML))\n",
    "    \n",
    "    model_list_lr = model_crossval.fit(df_interno)\n",
    "    \n",
    "    return model_list_lr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Executors for Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.functions import mean, col, lit\n",
    "\n",
    "def random_forest_executor(df_interno):\n",
    "    \n",
    "\n",
    "    rf = RandomForestRegressor(featuresCol = FEATURE_ML, labelCol = LABEL_ML , maxDepth=6 ,seed=42)\n",
    "    \n",
    "    stages_rf= enconder_indexer_stages(df_interno)\n",
    "    stages_rf = stages_rf+[rf] # append linear regression models as steps\n",
    "\n",
    "    pipeline_rf = Pipeline(stages = stages_rf)\n",
    "    \n",
    "    param_rf = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [10,100,1000]) \\\n",
    "        .build()\n",
    "    \n",
    "    \n",
    "    model_crossval = cross_validation(pipeline_rf, param_rf, RegressionEvaluator(metricName=METRIC_PERFORMANCE_ML))\n",
    "    \n",
    "    model_list_rf = model_crossval.fit(df_interno)\n",
    "    \n",
    "    return model_list_rf\n",
    "\n",
    "class DummieModel:\n",
    "    \n",
    "    def __init__(self,df_interno):\n",
    "        self.mean_label = df_interno.select( mean(col(LABEL_ML)).alias('mean')).collect()[0]['mean']\n",
    "        print(self.mean_label)\n",
    "    \n",
    "    def transform(self,df_interno):\n",
    "\n",
    "        return  df_interno.withColumn('prediction', lit(self.mean_label))\n",
    "    def save(self, folder):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for Organizing and Evaluating Models Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from itertools import cycle\n",
    "cycol = cycle('bgrcmk')\n",
    "\n",
    "def calc_rmse(label_col,df_predictions):\n",
    "    rf_evaluator = RegressionEvaluator(\n",
    "    labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = rf_evaluator.evaluate(df_predictions)\n",
    "    return rmse\n",
    "\n",
    "def calc_rsquare(label_col, df_predictions):\n",
    "    rf_evaluator = RegressionEvaluator(\n",
    "    labelCol=label_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    r2 = rf_evaluator.evaluate(df_predictions)\n",
    "    return r2\n",
    "    \n",
    "def rf_output(model_crossv_rf):\n",
    "    result={'model':\"rf\"}\n",
    "    result['r2_crossval']=str(model_crossv_rf.avgMetrics)\n",
    "    result['importance_list']=[]\n",
    "    #importance= model_crossv_rf.bestModel.stages[-1].featureImportances \n",
    "    #input_cols_rf = model_crossv_rf.bestModel.stages[-2].getInputCols()\n",
    "    #for x in range(len(input_cols_rf)):\n",
    "        #result['importance_list'].append({'feature':input_cols_rf[x],'importance':importance[x]})\n",
    "    return result\n",
    "\n",
    "def lr_output(model_crossv_lr):\n",
    "    result={'model':\"lr\"}\n",
    "    result['r2_crossval']=str(model_crossv_lr.avgMetrics)\n",
    "    lr_model= model_crossv_lr.bestModel.stages[-1] # extract linear regression model\n",
    "#     lr_summary = lr_model.summary\n",
    "#     result['rmse']=lr_summary.rootMeanSquaredError\n",
    "#     result['r2_summary']=lr_summary.r2\n",
    "    result['coefficients']= str(lr_model.coefficients)\n",
    "    result['intercept']= str(lr_model.intercept)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the scenery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if FEATURE_SELECTION_METHOD== FEATURE_SELECTION_PCA  or FEATURE_SELECTION_METHOD== FEATURE_SELECTION_ALL:\n",
    "    sceneries=[\n",
    "        df_dataset_final.columns\n",
    "    ]\n",
    "elif FEATURE_SELECTION_METHOD== FEATURE_SELECTION_NAIVE:\n",
    "    sceneries=[\n",
    "       selected_items_columns + [LABEL_ML]\n",
    "    ]\n",
    "    \n",
    "def run_sceneries(df_training, df_test):\n",
    "    \n",
    "    \n",
    "    lr_model_config = {'label':'lr', \"executor\":linear_regression_executor, \"output\":lr_output}\n",
    "    lr_model_ridge_config = {'label':'lr_ridge', \"executor\":ridge_regression_executor, \"output\":lr_output}\n",
    "    rf_model_ridge_config = {'label':'rf', \"executor\":random_forest_executor, \"output\":rf_output}\n",
    "    \n",
    "    baseline_model_config = {'label':'baseline', \"executor\":DummieModel, \"output\":rf_output}\n",
    "    \n",
    "#     if MODEL_TYPE == \"lr\":\n",
    "#         models = [baseline_model_config, lr_model_config, lr_model_ridge_config ]\n",
    "#     elif MODEL_TYPE == \"rf\":         \n",
    "#         models = [ rf_model_ridge_config]\n",
    "    models = [baseline_model_config, lr_model_config, lr_model_ridge_config , rf_model_ridge_config]\n",
    "    \n",
    "    models_final_result = {}\n",
    "    \n",
    "    plot_sampling_test = []\n",
    "        \n",
    "    SAMPLING_SIZE= 200  \n",
    "    \n",
    "    for local_model in models:\n",
    "        result = {'start':str(dt.datetime.now())}\n",
    "        model_result = local_model[\"executor\"](df_training)\n",
    "        \n",
    "        \n",
    "        result_model_training = model_result.transform(df_training)\n",
    "        result_model_test = model_result.transform(df_test)\n",
    "        \n",
    "\n",
    "        df_pd_result=result_model_test.select(LABEL_ML,\"prediction\").limit(SAMPLING_SIZE).toPandas()\n",
    "        df_pd_result.to_csv(FOLDER+'/%s_test_sampling.csv'%(local_model['label']), index = False, header=True)\n",
    "        \n",
    "        print(\"Calculating errors\")\n",
    "        result['r2_training']= calc_rsquare(LABEL_ML,result_model_training)\n",
    "        result['rmse_training']= calc_rmse(LABEL_ML,result_model_training)\n",
    "        result['r2_test']= calc_rsquare(LABEL_ML,result_model_test)\n",
    "        result['rmse_test']= calc_rmse(LABEL_ML,result_model_test)\n",
    "        \n",
    "        result['end']= str(dt.datetime.now())\n",
    "        \n",
    "#         explore_result = local_model[\"output\"](model_result)\n",
    "#         result.update(explore_result)\n",
    "        \n",
    "        models_final_result[local_model['label']]= result\n",
    "        \n",
    "        \n",
    "    #plot_errors_models_label(plot_sampling_test)\n",
    "    \n",
    "    \n",
    "    return models_final_result \n",
    "\n",
    "def run_all():\n",
    "    for sc in sceneries:\n",
    "\n",
    "        info_run=run_sceneries(training.select(sc), test.select(sc) )\n",
    "\n",
    "        result_local={'scenery':sc,'models_info':info_run}\n",
    "\n",
    "        result_notebook['sceneries'].append(result_local)\n",
    "\n",
    "        \n",
    "result_notebook['sceneries']=[]\n",
    "training, test = split_dataframe(df_dataset_final)\n",
    "\n",
    "run_all()\n",
    "\n",
    "result_notebook['end']=str(dt.datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results in a JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#result_notebook['step_wise_naive']=str(result_notebook['step_wise_naive'])\n",
    "print(json.dumps(result_notebook, indent=4,  sort_keys=True))\n",
    "\n",
    "with open(FOLDER+'/models_info_%s.json'%(MODEL_TYPE), 'w') as outfile:\n",
    "    json.dump(result_notebook, outfile, indent=4,  sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#result_local\n",
    "df_dataset_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_final.limit(20).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
