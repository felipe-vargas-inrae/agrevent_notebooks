{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración de datos para la colección de análisis de imágenes\n",
    "\n",
    "En el presente notebook se exploran los datos de la coleccion <i>phis_imagenanalysesangle_aggregated_by_day</i>, esta coleccion se encuentra en MongoDB y es el resultado de un preprocesamiento realizado con Pipelines de MongoDB. Este proceso se hizo como tranformación prevía para mejorar el rendimiento y reducir el dataset. La colección contiene documentos(registros),  cada documento de la colección contiene todos los datos que se recogieron de una variables en un solo dia, es decir, que los datos por horas fueron resumidos, en el registro se identifica el URI de la planta dentro del experimento y el seguimiento de cada variable. Para propósito de este caso de estudio los datos se agregan con la función promedio. \n",
    "\n",
    "\n",
    "## Configuración inicial \n",
    "\n",
    "En el siguiente bloque se cargan las librerías para hacer uso de pyspark, pyspark permite manipular el contexto de spark dentro de un programa de python, por ser este un contexto interactivo(Jupyter) se agrega la línea <i>findspark.init()</i>. Entre las configuraciones se destacan: la inclusión del conector mongodb para cargar colecciones, y la línea <i>.master('local[*]')</i> para habilitar en el contexto de pyspark la disponibilidad de todos los procesadores de la máquina\n",
    "\n",
    "*Nota: Previo a esta ejecución se debió descargar el driver del conector de mongo-spark la versión exacta se encuentra en el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MONGO_URI=\"mongodb://localhost:27017/iot_db\" \n",
    "my_spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .master('local[*]')\\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.3.2\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", MONGO_URI+\".phis_experiments\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#.config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/iot_db.performanceDataset\") \\\n",
    "#.config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/iot_db.result\") \\\n",
    "\n",
    "def getTableData(collection) : \n",
    "    return my_spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "                                            .option(\"database\",\"iot_db\")\\\n",
    "                                            .option(\"collection\", collection)\\\n",
    "                                            .load()\n",
    "\n",
    "#my_spark.sparkContext.getConf().getAll()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de experimentos\n",
    "\n",
    "En esta sesión se cargan los experimentos con el fin de tener en memoria una lista con los experimentos de la base de datos, un experimento es la entidad agrupadora de un grupo de plantas, o de el seguimiento en campo de varias áreas plantadas, todo el notebook está pensado para mirar solamente sobre un experimento\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`experimentURI`' given input columns: [];;\\n'Project ['experimentURI]\\n+- Relation[] MongoRelation(MongoRDD[0] at RDD at MongoRDD.scala:51,Some(StructType()))\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o49.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`experimentURI`' given input columns: [];;\n'Project ['experimentURI]\n+- Relation[] MongoRelation(MongoRDD[0] at RDD at MongoRDD.scala:51,Some(StructType()))\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:89)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3301)\r\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1312)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f0b5584cc430>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_experiments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"com.mongodb.spark.sql.DefaultSource\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_experiments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_experiments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'experimentURI'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_experiments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \"\"\"\n\u001b[1;32m-> 1202\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`experimentURI`' given input columns: [];;\\n'Project ['experimentURI]\\n+- Relation[] MongoRelation(MongoRDD[0] at RDD at MongoRDD.scala:51,Some(StructType()))\\n\""
     ]
    }
   ],
   "source": [
    "df_experiments = my_spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "df_experiments=df_experiments.select('experimentURI')\n",
    "df_experiments.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de la colección imagenanalysesangle_aggregated_by_day\n",
    "\n",
    "En este punto se hace  un llamado a una función ya definida que permite cargar una colección de mongodb a un dataframe, después se proyectan los atributos de interés, finalmente se hace uso de la función <b>pivot</b> de pyspark que permite convertir cada valor distinto dentro de una columna en otra columna, cambiando así la forma del dataset.\n",
    "\n",
    "Dado que <i>variableCode</i> contiene una lista arbitraria de nombres de variables esta línea de código aporta flexibilidad y hace una reestructuración automática del dataset, la forma de la coleccion de entrada está pensada para facilitar la ejecución de este método.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_anaysis = getTableData( \"phis_imagenanalysesangle_aggregated_by_day\")\n",
    "\n",
    "print(\"Se pueden observar algunas estadísticas\")\n",
    "print(\"# de registros\",df_images_anaysis.count())\n",
    "print(\"# de Plantas\",df_images_anaysis.select(\"plantURI\").distinct().count())\n",
    "print(\"Variables iniciales\")\n",
    "df_images_anaysis.select(\"variableCode\").distinct().show()\n",
    "\n",
    "your_min_value = df_images_anaysis.agg({\"dayOfYear\": \"min\"}).collect()[0][0]\n",
    "print(\"Initial Day:\",your_min_value)\n",
    "your_max_value = df_images_anaysis.agg({\"dayOfYear\": \"max\"}).collect()[0][0]\n",
    "print(\"End Day:\", your_max_value)\n",
    "\n",
    "print(\"Se pueden observar los valores de la columna variableCode\")\n",
    "df_images_anaysis.select(\"plantURI\",\"dayOfYear\", \"variableCode\", \"variableCodeValueAvg\").show(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_anaysis = df_images_anaysis.select(\"plantURI\", \"dayOfYear\", \"variableCode\", \"variableCodeValueAvg\")\n",
    "df_grp_img_analysis = df_images_anaysis.groupBy(\"plantURI\", \"dayOfYear\").pivot(\"variableCode\").mean(\"variableCodeValueAvg\")\n",
    "\n",
    "print(\"Se pueden observar dos de las nuevas columnas\")\n",
    "df_grp_img_analysis.select(\"plantURI\", \"dayOfYear\",\"height_over_pot\",\"convex_hull_area\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EL tiempo de ejecución de la función pivot fue de 56s y se ejecutaron 200 tareas en spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df_grp_img_analysis.describe().show(truncate=False, vertical=True)\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame()\n",
    "def describe_pd(df_in, columns, deciles=False):\n",
    "    '''\n",
    "    Function to union the basic stats results and deciles\n",
    "    :param df_in: the input dataframe\n",
    "    :param columns: the cloumn name list of the numerical variable\n",
    "    :param deciles: the deciles output\n",
    "\n",
    "    :return : the numerical describe info. of the input dataframe\n",
    "\n",
    "    :author: Ming Chen and Wenqiang Feng\n",
    "    :email:  von198@gmail.com\n",
    "    '''\n",
    "\n",
    "    if deciles:\n",
    "        percentiles = np.array(range(0, 110, 10))\n",
    "    else:\n",
    "        percentiles = [0.25, 0.5,0.75]\n",
    "\n",
    "    error=0.1\n",
    "    percs = np.transpose([df_in.approxQuantile(x,percentiles, error) for x in columns])\n",
    "    percs = pd.DataFrame(percs, columns=columns)\n",
    "    percs['summary'] = [str(p) + '%' for p in percentiles]\n",
    "\n",
    "    spark_describe = df_in.describe().toPandas()\n",
    "    new_df = pd.concat([spark_describe, percs],ignore_index=True, sort=True)\n",
    "    new_df = new_df.round(2)\n",
    "    return new_df[['summary'] + columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción Estadística de las Columnas Numéricas\n",
    "\n",
    "El dataset formado de filas y columnas se encuentra en el contexto de pyspark, en el código de arriba se implementa una función para imprimir datos estadísticos de cada variable. La función describe() de spark retorna algunos datos estadísticos importantes sin embargo no muestra los cuartiles, es por ello que se debe usar approxQuantile una función propia de spark que recibe la columna, el percentil de interés y una fracción de error permitido, puesto que para cantidades de datos elevadas el cálculo es aproximado. Es de notar que la función approxQuantile es una función nativa de pyspark, a pesar de ser llamada dentro de python el grueso de su ejecución tendrá lugar dentro de la JVM(Java Virtual Machine). La recomendación es utilizar con prioridad las funciones propias de spark y no combinarlo tanto con numpy o pandas, a menos de que ser necesario, aunque las librerías de python tengan mejor documentación y mayor cantidad de funciones. Esta ejecución es exhaustiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#df_grp_img_analysis_sample= df_grp_img_analysis.sample()\n",
    "num_cols= ['dayOfYear','Silk_area','convex_hull_area','convex_hull_perimeter','height','height_over_pot','height_under_pot','number_of_objects','object_sum_area','width']\n",
    "\n",
    "\n",
    "df_stats=describe_pd(df_grp_img_analysis, num_cols)\n",
    "\n",
    "df_stats\n",
    "#df_grp_img_analysis_sample.approxQuantile(\"Silk_area\",[0.25, 0.5,0.75], 0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Este código ejecuta varios llamados de la función approxQuantile uno por variable para calcular: la mediana, q1 y q3, un total de 12 Jobs fueron ejecutados, 417 tareas por job, el tiempo de ejecución para cada job fue de 1min aprox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box plot y big dataset \n",
    "\n",
    "En el siguiente código se va a crear un pequeño dataset para visualizar un Box Plot representativo de los datos del big dataset que se encuentra en spark, dado que no es fácil instanciar una herramienta de visualización con el big dataset lo que se hizo  fue calcular los estadísticos necesarios y crear un dataset con la forma: \n",
    "\n",
    "<code> [min,25%, 25% ,median, 75%, 75% ,max]</code>\n",
    "\n",
    "En base a este formato se crearon los boxplot para cada variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reshaping statistics to draw box plot\n",
    "\n",
    "toDelete=[\"count\", \"mean\", \"stddev\"]\n",
    "tReplicate=[\"0.25%\", \"0.75%\"]\n",
    "\n",
    "## remove count mean and std\n",
    "df_stats_temp= df_stats[~df_stats[\"summary\"].isin(toDelete) ]\n",
    "\n",
    "## get quartile .75 and .25\n",
    "df_stats_dup= df_stats[df_stats[\"summary\"].isin(tReplicate) ]\n",
    "\n",
    "##replicate quartiles\n",
    "df_stats_temp=df_stats_temp.append(df_stats_dup, ignore_index = True) \n",
    "\n",
    "df_stats_temp=df_stats_temp.drop(\"summary\",axis=1)\n",
    "\n",
    "# df_stats_temp.dtypes\n",
    "df_stats_temp=df_stats_temp.astype('float')\n",
    "\n",
    "# df_stats_temp.dtypes\n",
    "\n",
    "# df_stats_temp=df_stats_temp.sort_values('dayOfYear')\n",
    "\n",
    "print(df_stats_temp)\n",
    "print(df_stats_temp.dtypes)\n",
    "print(df_stats_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaciones \n",
    "\n",
    "Para la visualización de datos se va a usar la librería plotly, esta es una librería javascript con funcionalidades para python que se puede utilizar dentro de los notebook, ofrece gráficos interactivos y crea diferentes tipo de render como imágenes:png, svg, documentos HTMLs interactivos, entre otros. Esta misma librería se puede usar para crear una aplicación web \n",
    "\n",
    "En el siguiente bloque se cargan las librerías de visualización "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import plotly\n",
    "\n",
    "import plotly.plotly as pl\n",
    "import cufflinks as cf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(cf.__version__)\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode,plot,iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Plot Graphic\n",
    "\n",
    "Una vez cargada la librería se presenta esta herramienta visual estadísticas, se usó la funcionalidad de subplots por que cada atributo tiene unidades distintas y aveces se requiere saber el max y el min de una variable, y reconocer donde se encuentra la mayor densidad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=[min, q1, q1, median, q3, q3, max]\n",
    "\n",
    "#normalized_df=(df_stats_temp-df_stats_temp.mean())/df_stats_temp.std()\n",
    "df_stats_temp.iplot(kind=\"box\",subplots=True, shape=(2,5), shared_xaxes=True, fill=True,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muestreo de Datos \n",
    "\n",
    "En el siguiente bloque se retorna un muestreo aleatorio del dataset grande  para visualizar los datos, dado que spark no cuenta con una libreria gratuita de visualización esta es una forma util y eficiente de visualizar los datos y justifica el uso de python. La función recibe tres parametros un boolean que indica que no actualice el dataframe original, una porcion deseada, una semilla de aleatoriedad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/cameres/bc24ac6711c9e537dd20be47b2a83558\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_view_vars_temp= df_grp_img_analysis.sample(False, 0.1, 12345).toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Este código tardó aprox. 1 min y se ejecutaron 414 tareas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización por planta\n",
    "\n",
    "La siguiente función toma 3 plantas de la lista de plantas y para cada una hace una gráfica con todas las variables de estudio, estas variables fueron normalizadas para que compartieran los mismo ejes, ver tendencias conjuntas y correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_view_vars_temp.columns)\n",
    "df_view_vars= df_view_vars_temp#.set_index([\"plantURI\",\"dayOfYear\"])\n",
    "\n",
    "#df_view_vars=df_view_vars.sort_index()\n",
    "\n",
    "\n",
    "\n",
    "#listof_plants= df_view_vars.index.get_level_values('plantURI').unique().tolist()\n",
    "listof_plants= df_view_vars['plantURI'].unique().tolist()\n",
    "\n",
    "print(\"Number of plants\",len(listof_plants))\n",
    "\n",
    "listof_plants_first_3=listof_plants[:3]\n",
    "\n",
    "\n",
    "# #df_view_vars.iplot(kind='scatter', filename='cufflinks/cf-simple-line')\n",
    "\n",
    "# #from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "for plant in listof_plants_first_3:\n",
    "    dataDf=df_view_vars[df_view_vars['plantURI']==plant]\n",
    "    dataDf.index= dataDf[\"dayOfYear\"]\n",
    "    dataDf=dataDf.sort_index()\n",
    "    dataDf=dataDf.drop(['plantURI', 'dayOfYear','Silk_area'], axis=1)\n",
    "    \n",
    "    dataDf=(dataDf-dataDf.min())/dataDf.max()  # normalization\n",
    "    dataDf=dataDf.round(2) \n",
    "    \n",
    "    print('Plant URI:',plant)\n",
    "    dataDf.iplot(kind='scatter', filename='cufflinks/cf-simple-line')\n",
    "    #print(dataDf.head(20))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización: Plantas vs Variable\n",
    "\n",
    "En las siguientes se toman algunas variables y se elabora un plot con cada una de ellas, el plot contiene el comportamienta de esa variable para cada planta, por razones de rendimiento se hacen visible las primeras diez plantas, es posible buscar la planta de interes y dar click sobre el legend para mostrarla o desvanecerla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## a graphic by variable for all the plants\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "df_view_one_var=df_view_vars_temp\n",
    "\n",
    "columns= [\"convex_hull_area\", \"width\", \"height\",\"number_of_objects\"]\n",
    "\n",
    "#columns= [ \"width\"]\n",
    "\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=go.layout.Title(\n",
    "        text='Plot Title',\n",
    "        xref='paper',\n",
    "        x=0\n",
    "    ),\n",
    "    xaxis=go.layout.XAxis(\n",
    "        title=go.layout.xaxis.Title(\n",
    "            text='Time',\n",
    "            font=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=18,\n",
    "                color='#7f7f7f'\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    yaxis=go.layout.YAxis(\n",
    "        title=go.layout.yaxis.Title(\n",
    "            text='Amount',\n",
    "            font=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=18,\n",
    "                color='#7f7f7f'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "for col in columns:\n",
    "    plt_settings=[]\n",
    "    for index, plant in enumerate(listof_plants):\n",
    "        dataDf=df_view_one_var[df_view_one_var[\"plantURI\"]==plant]\n",
    "        dataDf.index= dataDf[\"dayOfYear\"]\n",
    "        dataDf=dataDf.sort_index()\n",
    "        \n",
    "        visibility = \"legendonly\" if index > 10 else True\n",
    "        config= {'x':dataDf.index, 'y':dataDf[col], 'name':plant , 'visible':visibility}\n",
    "        plt_settings.append(config)\n",
    "    # plot the graphics\n",
    "    layout.title.text=\"All Plants Behavior in Time for \"+col\n",
    "    fig = go.Figure(data=plt_settings, layout=layout)\n",
    "    iplot( fig, filename='cufflinks/simple-line')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de la gráfica\n",
    "\n",
    "A partir de este gráfico se pueden sacar algunas conclusiones, en general no todas las plantas tiene seguimiento para todos los días del experimentos es por ello que se observan lineas que no llegan hasta el último día.\n",
    "\n",
    "* <b>Convext Hull Area:</b>  esta variable es el cálculo del área del poligono más pequeno que contiene todos los puntos de la imagen, esta variable muestra una pendiente positiva lo que se relaciona con el crecimiento de la planta, se observa como la variabilidad se empieza a notar desde el día 124, es en este punto que cada planta demuestra su potencial en cuanto a crecimiento.\n",
    "\n",
    "* <b>Width:</b> Esta variable representa el máximo ancho de la planta la distancía entre la hoja más lejana a la izquierda y a la derecha, se observa una tendencia positiva pero en ocasiones tambien se evidencia que no hay crecimiento sino que se mantiene un ancho en el tiempo, aveces se observa un decenso de la linea pero este no es tan brusco, la planta puede crecer y las hojas cambiar de posición\n",
    "\n",
    "* <b>Height:</b> Esta variable representa el punto más alto de la planta medido desde el punto más alto de la matera, a diferencia del width este valor muestra una tendencia positiva constante no es normal que la planta pierda alto durante el crecimiento almenos para este cultivar\n",
    "\n",
    "* <b>Number of objects:</b> Esta variable representa el número de hojas, su exactitud es afectada fuertemente por la calidad del algoritmo para el conteo, esta es una tarea de procesamiento de imágenes que no es fácil se observan picos altos y descensos en toda la gráfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Información Faltante\n",
    "\n",
    "Algunas plantas no llevaron seguimiento los 73 días del experimento la siguiente gráfica sirve para entender el grueso de las plantas cuantos días tuvieron seguimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "df_nulls=df_grp_img_analysis.groupBy('plantURI').agg(count(\"*\").alias(\"RecordsNumber\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El tiempo de ejecución de este Job fue de 3min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nulls[\"RecordsNumber\"].iplot(kind='histogram', filename='cufflinks/basic-histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este histograma indica el número de registros por planta. Se evidencía con este histograma que el experimento se llevo a cabo en dos etapas, y eso podría explicar la duración distinta de cada siguimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
